/* VECTORBLOX MXP SOFTWARE DEVELOPMENT KIT
 *
 * Copyright (C) 2012-2016 VectorBlox Computing Inc., Vancouver, British Columbia, Canada.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *
 *     * Neither the name of VectorBlox Computing Inc. nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * This agreement shall be governed in all respects by the laws of the Province
 * of British Columbia and by the laws of Canada.
 *
 * This file is part of the VectorBlox MXP Software Development Kit.
 *
 */

#ifndef __VBXX_HPP
#define __VBXX_HPP

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVB, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm( VVB, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm( VVB, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm( VVB, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm( VVB, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm( VVB, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm( VVB, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm( VVB, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm( VVB, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm( VVB, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm( VVB, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm( VVB, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm( VVB, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm( VVB, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm( VVB, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm( VVB, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VVB, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm( VVB, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm( VVB, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm( VVB, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm( VVB, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm( VVB, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm( VVB, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm( VVB, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm( VVB, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm( VVB, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm( VVB, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm( VVB, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm( VVB, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm( VVB, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm( VVB, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm( VVB, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm( VVBU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm( VVBU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm( VVBU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm( VVBU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm( VVBU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm( VVBU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm( VVBU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm( VVBU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm( VVBU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm( VVBU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm( VVBU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm( VVBU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm( VVBU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm( VVBU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VVBU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm( VVBU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm( VVBU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm( VVBU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm( VVBU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm( VVBU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm( VVBU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm( VVBU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm( VVBU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm( VVBU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm( VVBU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm( VVBU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm( VVBU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm( VVBU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm( VVBU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm( VVBU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVH, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm( VVH, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm( VVH, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm( VVH, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm( VVH, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm( VVH, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm( VVH, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm( VVH, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm( VVH, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm( VVH, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm( VVH, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm( VVH, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm( VVH, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm( VVH, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm( VVH, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm( VVH, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VVH, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm( VVH, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm( VVH, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm( VVH, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm( VVH, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm( VVH, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm( VVH, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm( VVH, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm( VVH, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm( VVH, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm( VVH, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm( VVH, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm( VVH, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm( VVH, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm( VVH, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm( VVH, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm( VVHU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm( VVHU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm( VVHU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm( VVHU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm( VVHU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm( VVHU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm( VVHU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm( VVHU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm( VVHU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm( VVHU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm( VVHU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm( VVHU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm( VVHU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm( VVHU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VVHU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm( VVHU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm( VVHU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm( VVHU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm( VVHU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm( VVHU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm( VVHU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm( VVHU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm( VVHU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm( VVHU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm( VVHU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm( VVHU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm( VVHU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm( VVHU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm( VVHU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm( VVHU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVW, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm( VVW, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm( VVW, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm( VVW, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm( VVW, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm( VVW, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm( VVW, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm( VVW, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm( VVW, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm( VVW, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm( VVW, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm( VVW, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm( VVW, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm( VVW, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm( VVW, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm( VVW, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VVW, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm( VVW, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm( VVW, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm( VVW, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm( VVW, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm( VVW, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm( VVW, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm( VVW, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm( VVW, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm( VVW, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm( VVW, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm( VVW, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm( VVW, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm( VVW, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm( VVW, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm( VVW, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm( VVWU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm( VVWU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm( VVWU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm( VVWU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm( VVWU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm( VVWU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm( VVWU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm( VVWU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm( VVWU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm( VVWU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm( VVWU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm( VVWU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm( VVWU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm( VVWU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VVWU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm( VVWU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm( VVWU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm( VVWU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm( VVWU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm( VVWU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm( VVWU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm( VVWU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm( VVWU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm( VVWU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm( VVWU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm( VVWU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm( VVWU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm( VVWU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm( VVWU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm( VVWU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVB, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm( SVB, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm( SVB, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm( SVB, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm( SVB, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm( SVB, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm( SVB, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm( SVB, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm( SVB, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm( SVB, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm( SVB, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm( SVB, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm( SVB, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm( SVB, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm( SVB, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm( SVB, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SVB, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm( SVB, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm( SVB, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm( SVB, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm( SVB, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm( SVB, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm( SVB, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm( SVB, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm( SVB, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm( SVB, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm( SVB, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm( SVB, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm( SVB, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm( SVB, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm( SVB, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm( SVB, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm( SVBU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm( SVBU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm( SVBU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm( SVBU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm( SVBU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm( SVBU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm( SVBU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm( SVBU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm( SVBU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm( SVBU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm( SVBU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm( SVBU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm( SVBU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm( SVBU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SVBU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm( SVBU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm( SVBU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm( SVBU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm( SVBU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm( SVBU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm( SVBU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm( SVBU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm( SVBU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm( SVBU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm( SVBU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm( SVBU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm( SVBU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm( SVBU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm( SVBU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm( SVBU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVH, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm( SVH, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm( SVH, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm( SVH, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm( SVH, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm( SVH, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm( SVH, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm( SVH, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm( SVH, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm( SVH, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm( SVH, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm( SVH, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm( SVH, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm( SVH, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm( SVH, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm( SVH, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SVH, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm( SVH, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm( SVH, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm( SVH, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm( SVH, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm( SVH, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm( SVH, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm( SVH, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm( SVH, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm( SVH, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm( SVH, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm( SVH, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm( SVH, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm( SVH, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm( SVH, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm( SVH, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm( SVHU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm( SVHU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm( SVHU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm( SVHU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm( SVHU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm( SVHU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm( SVHU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm( SVHU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm( SVHU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm( SVHU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm( SVHU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm( SVHU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm( SVHU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm( SVHU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SVHU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm( SVHU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm( SVHU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm( SVHU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm( SVHU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm( SVHU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm( SVHU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm( SVHU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm( SVHU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm( SVHU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm( SVHU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm( SVHU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm( SVHU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm( SVHU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm( SVHU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm( SVHU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVW, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm( SVW, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm( SVW, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm( SVW, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm( SVW, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm( SVW, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm( SVW, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm( SVW, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm( SVW, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm( SVW, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm( SVW, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm( SVW, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm( SVW, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm( SVW, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm( SVW, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm( SVW, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SVW, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm( SVW, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm( SVW, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm( SVW, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm( SVW, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm( SVW, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm( SVW, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm( SVW, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm( SVW, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm( SVW, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm( SVW, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm( SVW, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm( SVW, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm( SVW, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm( SVW, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm( SVW, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm( SVWU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm( SVWU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm( SVWU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm( SVWU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm( SVWU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm( SVWU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm( SVWU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm( SVWU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm( SVWU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm( SVWU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm( SVWU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm( SVWU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm( SVWU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm( SVWU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SVWU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm( SVWU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm( SVWU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm( SVWU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm( SVWU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm( SVWU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm( SVWU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm( SVWU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm( SVWU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm( SVWU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm( SVWU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm( SVWU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm( SVWU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm( SVWU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm( SVWU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm( SVWU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEB, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEB, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEB, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEB, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEB, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEB, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEB, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEB, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm( VEB, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm( VEB, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm( VEB, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEB, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm( VEB, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm( VEB, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm( VEB, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm( VEB, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm( VEB, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm( VEB, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm( VEB, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm( VEB, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm( VEB, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm( VEB, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEB, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEB, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEB, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEB, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEB, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEB, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEB, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEB, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEB, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEB, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEB, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEB, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEB, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEB, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEB, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEB, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm( VEBU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm( VEBU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm( VEBU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm( VEBU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm( VEBU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm( VEBU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm( VEBU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm( VEBU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm( VEBU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm( VEBU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm( VEBU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm( VEBU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEH, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEH, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEH, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEH, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEH, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEH, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEH, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEH, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm( VEH, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm( VEH, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm( VEH, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEH, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm( VEH, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm( VEH, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm( VEH, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm( VEH, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm( VEH, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm( VEH, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm( VEH, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm( VEH, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm( VEH, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm( VEH, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEH, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEH, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEH, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEH, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEH, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEH, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEH, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEH, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEH, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEH, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEH, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEH, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEH, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEH, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEH, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEH, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm( VEHU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm( VEHU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm( VEHU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm( VEHU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm( VEHU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm( VEHU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm( VEHU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm( VEHU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm( VEHU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm( VEHU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm( VEHU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm( VEHU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEW, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEW, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEW, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEW, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEW, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEW, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEW, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEW, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm( VEW, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm( VEW, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm( VEW, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEW, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm( VEW, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm( VEW, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm( VEW, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm( VEW, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm( VEW, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm( VEW, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm( VEW, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm( VEW, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm( VEW, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm( VEW, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEW, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEW, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEW, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEW, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEW, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEW, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEW, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEW, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEW, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEW, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEW, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEW, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEW, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEW, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEW, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEW, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm( VEWU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm( VEWU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm( VEWU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm( VEWU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm( VEWU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm( VEWU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm( VEWU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm( VEWU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm( VEWU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm( VEWU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm( VEWU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm( VEWU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SEB, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEB, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEB, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEB, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEB, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEB, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEB, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEB, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEB, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEB, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEB, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEB, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEB, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEB, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEB, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEB, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEB, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEB, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEB, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEB, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEB, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEB, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEB, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEB, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEB, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEB, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEB, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEB, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEB, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEB, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEB, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEB, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEB, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEB, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEB, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEB, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEB, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SEH, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEH, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEH, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEH, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEH, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEH, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEH, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEH, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEH, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEH, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEH, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEH, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEH, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEH, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEH, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEH, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEH, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEH, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEH, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEH, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEH, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEH, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEH, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEH, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEH, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEH, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEH, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEH, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEH, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEH, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEH, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEH, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEH, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEH, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEH, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEH, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEH, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SEW, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEW, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEW, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEW, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEW, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEW, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEW, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEW, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEW, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEW, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEW, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEW, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEW, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEW, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEW, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEW, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEW, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEW, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEW, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEW, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEW, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEW, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEW, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEW, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEW, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEW, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEW, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEW, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEW, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEW, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEW, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEW, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEW, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEW, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEW, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEW, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEW, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVB, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked( VVB, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked( VVB, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked( VVB, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVB, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked( VVB, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked( VVB, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked( VVB, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked( VVB, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked( VVB, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked( VVB, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked( VVB, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked( VVB, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked( VVB, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked( VVB, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked( VVB, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVB, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVB, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVB, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVB, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVB, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVB, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVB, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVB, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVB, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVB, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVB, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVB, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVB, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVB, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVB, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVB, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked( VVBU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked( VVBU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked( VVBU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked( VVBU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked( VVBU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked( VVBU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked( VVBU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked( VVBU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked( VVBU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked( VVBU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked( VVBU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked( VVBU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVH, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked( VVH, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked( VVH, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked( VVH, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVH, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked( VVH, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked( VVH, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked( VVH, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked( VVH, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked( VVH, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked( VVH, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked( VVH, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked( VVH, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked( VVH, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked( VVH, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked( VVH, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVH, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVH, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVH, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVH, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVH, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVH, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVH, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVH, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVH, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVH, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVH, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVH, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVH, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVH, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVH, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVH, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked( VVHU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked( VVHU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked( VVHU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked( VVHU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked( VVHU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked( VVHU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked( VVHU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked( VVHU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked( VVHU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked( VVHU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked( VVHU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked( VVHU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVW, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked( VVW, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked( VVW, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked( VVW, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVW, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked( VVW, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked( VVW, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked( VVW, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked( VVW, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked( VVW, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked( VVW, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked( VVW, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked( VVW, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked( VVW, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked( VVW, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked( VVW, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVW, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVW, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVW, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVW, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVW, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVW, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVW, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVW, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVW, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVW, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVW, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVW, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVW, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVW, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVW, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVW, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked( VVWU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked( VVWU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked( VVWU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked( VVWU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked( VVWU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked( VVWU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked( VVWU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked( VVWU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked( VVWU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked( VVWU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked( VVWU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked( VVWU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVB, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked( SVB, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked( SVB, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked( SVB, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked( SVB, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked( SVB, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked( SVB, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked( SVB, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked( SVB, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked( SVB, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked( SVB, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked( SVB, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked( SVB, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked( SVB, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked( SVB, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked( SVB, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVB, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVB, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVB, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVB, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVB, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVB, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVB, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVB, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVB, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVB, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVB, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVB, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVB, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVB, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVB, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVB, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked( SVBU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked( SVBU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked( SVBU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked( SVBU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked( SVBU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked( SVBU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked( SVBU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked( SVBU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked( SVBU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked( SVBU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked( SVBU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked( SVBU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked( SVBU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVH, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked( SVH, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked( SVH, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked( SVH, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked( SVH, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked( SVH, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked( SVH, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked( SVH, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked( SVH, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked( SVH, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked( SVH, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked( SVH, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked( SVH, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked( SVH, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked( SVH, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked( SVH, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVH, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVH, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVH, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVH, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVH, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVH, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVH, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVH, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVH, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVH, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVH, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVH, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVH, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVH, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVH, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVH, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked( SVHU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked( SVHU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked( SVHU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked( SVHU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked( SVHU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked( SVHU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked( SVHU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked( SVHU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked( SVHU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked( SVHU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked( SVHU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked( SVHU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked( SVHU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVW, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked( SVW, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked( SVW, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked( SVW, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked( SVW, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked( SVW, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked( SVW, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked( SVW, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked( SVW, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked( SVW, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked( SVW, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked( SVW, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked( SVW, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked( SVW, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked( SVW, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked( SVW, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVW, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVW, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVW, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVW, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVW, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVW, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVW, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVW, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVW, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVW, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVW, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVW, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVW, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVW, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVW, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVW, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked( SVWU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked( SVWU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked( SVWU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked( SVWU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked( SVWU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked( SVWU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked( SVWU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked( SVWU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked( SVWU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked( SVWU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked( SVWU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked( SVWU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked( SVWU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEB, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEB, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEB, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEB, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEB, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEB, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEB, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEB, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEB, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEB, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEB, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEB, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEB, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEB, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEB, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked( VEB, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked( VEB, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEB, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEB, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEB, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEB, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEB, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEB, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEB, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEB, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEB, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEB, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEB, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEB, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEB, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEB, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEB, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEB, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEB, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEB, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEB, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEB, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEB, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEH, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEH, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEH, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEH, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEH, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEH, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEH, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEH, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEH, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEH, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEH, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEH, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEH, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEH, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEH, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked( VEH, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked( VEH, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEH, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEH, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEH, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEH, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEH, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEH, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEH, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEH, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEH, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEH, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEH, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEH, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEH, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEH, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEH, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEH, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEH, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEH, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEH, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEH, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEH, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEW, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEW, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEW, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEW, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEW, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEW, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEW, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEW, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEW, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEW, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEW, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEW, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEW, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEW, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEW, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked( VEW, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked( VEW, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEW, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEW, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEW, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEW, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEW, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEW, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEW, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEW, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEW, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEW, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEW, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEW, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEW, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEW, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEW, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEW, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEW, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEW, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEW, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEW, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEW, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEB, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEB, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEB, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEB, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEB, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEB, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEB, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEB, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEB, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEB, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEB, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEB, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEB, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEB, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEB, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEB, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEB, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEB, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEB, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEB, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEB, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEB, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEB, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEB, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEB, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEB, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEB, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEB, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEB, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEB, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEB, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEB, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEB, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEB, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEB, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEB, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEB, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEH, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEH, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEH, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEH, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEH, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEH, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEH, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEH, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEH, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEH, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEH, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEH, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEH, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEH, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEH, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEH, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEH, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEH, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEH, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEH, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEH, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEH, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEH, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEH, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEH, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEH, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEH, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEH, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEH, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEH, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEH, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEH, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEH, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEH, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEH, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEH, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEH, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEW, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEW, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEW, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEW, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEW, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEW, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEW, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEW, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEW, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEW, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEW, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEW, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEW, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEW, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEW, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEW, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEW, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEW, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEW, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEW, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEW, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEW, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEW, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEW, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEW, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEW, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEW, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEW, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEW, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEW, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEW, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEW, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEW, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEW, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEW, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEW, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEW, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVB, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc( VVB, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc( VVB, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc( VVB, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVB, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc( VVB, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc( VVB, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc( VVB, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc( VVB, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc( VVB, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc( VVB, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc( VVB, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc( VVB, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc( VVB, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc( VVB, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc( VVB, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVB, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVB, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVB, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVB, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVB, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVB, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVB, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVB, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVB, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVB, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVB, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVB, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVB, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVB, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVB, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVB, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc( VVBU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc( VVBU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc( VVBU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc( VVBU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc( VVBU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc( VVBU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc( VVBU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc( VVBU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc( VVBU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc( VVBU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc( VVBU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc( VVBU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVH, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc( VVH, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc( VVH, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc( VVH, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVH, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc( VVH, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc( VVH, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc( VVH, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc( VVH, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc( VVH, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc( VVH, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc( VVH, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc( VVH, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc( VVH, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc( VVH, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc( VVH, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVH, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVH, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVH, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVH, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVH, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVH, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVH, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVH, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVH, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVH, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVH, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVH, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVH, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVH, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVH, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVH, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc( VVHU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc( VVHU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc( VVHU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc( VVHU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc( VVHU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc( VVHU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc( VVHU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc( VVHU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc( VVHU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc( VVHU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc( VVHU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc( VVHU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVW, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc( VVW, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc( VVW, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc( VVW, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVW, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc( VVW, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc( VVW, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc( VVW, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc( VVW, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc( VVW, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc( VVW, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc( VVW, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc( VVW, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc( VVW, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc( VVW, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc( VVW, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVW, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVW, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVW, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVW, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVW, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVW, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVW, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVW, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVW, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVW, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVW, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVW, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVW, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVW, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVW, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVW, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc( VVWU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc( VVWU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc( VVWU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc( VVWU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc( VVWU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc( VVWU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc( VVWU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc( VVWU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc( VVWU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc( VVWU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc( VVWU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc( VVWU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVB, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc( SVB, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc( SVB, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc( SVB, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc( SVB, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc( SVB, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc( SVB, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc( SVB, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc( SVB, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc( SVB, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc( SVB, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc( SVB, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc( SVB, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc( SVB, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc( SVB, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc( SVB, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVB, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVB, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVB, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVB, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVB, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVB, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVB, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVB, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVB, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVB, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVB, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVB, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVB, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVB, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVB, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVB, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc( SVBU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc( SVBU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc( SVBU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc( SVBU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc( SVBU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc( SVBU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc( SVBU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc( SVBU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc( SVBU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc( SVBU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc( SVBU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc( SVBU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc( SVBU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVH, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc( SVH, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc( SVH, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc( SVH, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc( SVH, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc( SVH, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc( SVH, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc( SVH, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc( SVH, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc( SVH, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc( SVH, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc( SVH, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc( SVH, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc( SVH, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc( SVH, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc( SVH, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVH, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVH, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVH, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVH, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVH, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVH, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVH, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVH, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVH, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVH, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVH, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVH, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVH, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVH, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVH, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVH, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc( SVHU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc( SVHU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc( SVHU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc( SVHU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc( SVHU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc( SVHU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc( SVHU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc( SVHU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc( SVHU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc( SVHU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc( SVHU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc( SVHU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc( SVHU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVW, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc( SVW, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc( SVW, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc( SVW, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc( SVW, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc( SVW, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc( SVW, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc( SVW, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc( SVW, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc( SVW, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc( SVW, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc( SVW, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc( SVW, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc( SVW, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc( SVW, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc( SVW, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVW, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVW, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVW, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVW, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVW, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVW, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVW, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVW, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVW, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVW, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVW, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVW, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVW, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVW, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVW, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVW, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc( SVWU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc( SVWU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc( SVWU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc( SVWU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc( SVWU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc( SVWU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc( SVWU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc( SVWU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc( SVWU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc( SVWU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc( SVWU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc( SVWU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc( SVWU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEB, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEB, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEB, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEB, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEB, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEB, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEB, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEB, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEB, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEB, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEB, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEB, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEB, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEB, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEB, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc( VEB, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc( VEB, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEB, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEB, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEB, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEB, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEB, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEB, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEB, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEB, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEB, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEB, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEB, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEB, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEB, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEB, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEB, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEB, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEB, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEB, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEB, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEB, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEB, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEH, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEH, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEH, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEH, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEH, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEH, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEH, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEH, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEH, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEH, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEH, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEH, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEH, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEH, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEH, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc( VEH, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc( VEH, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEH, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEH, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEH, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEH, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEH, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEH, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEH, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEH, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEH, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEH, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEH, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEH, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEH, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEH, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEH, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEH, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEH, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEH, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEH, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEH, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEH, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEW, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEW, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEW, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEW, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEW, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEW, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEW, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEW, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEW, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEW, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEW, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEW, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEW, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEW, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEW, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc( VEW, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc( VEW, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEW, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEW, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEW, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEW, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEW, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEW, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEW, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEW, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEW, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEW, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEW, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEW, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEW, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEW, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEW, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEW, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEW, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEW, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEW, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEW, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEW, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEB, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEB, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEB, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEB, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEB, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEB, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEB, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEB, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEB, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEB, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEB, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEB, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEB, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEB, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEB, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEB, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEB, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEB, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEB, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEB, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEB, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEB, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEB, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEB, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEB, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEB, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEB, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEB, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEB, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEB, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEB, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEB, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEB, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEB, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEB, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEB, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEB, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEH, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEH, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEH, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEH, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEH, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEH, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEH, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEH, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEH, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEH, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEH, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEH, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEH, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEH, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEH, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEH, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEH, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEH, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEH, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEH, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEH, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEH, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEH, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEH, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEH, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEH, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEH, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEH, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEH, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEH, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEH, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEH, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEH, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEH, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEH, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEH, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEH, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEW, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEW, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEW, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEW, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEW, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEW, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEW, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEW, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEW, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEW, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEW, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEW, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEW, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEW, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEW, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEW, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEW, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEW, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEW, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEW, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEW, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEW, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEW, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEW, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEW, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEW, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEW, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEW, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEW, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEW, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEW, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEW, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEW, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEW, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEW, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEW, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEW, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVB, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVB, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVB, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVB, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVB, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVB, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVB, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVB, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked_acc( VVB, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked_acc( VVB, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVB, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVB, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVB, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVB, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVB, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked_acc( VVB, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVB, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVB, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVB, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVB, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVB, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVB, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVB, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVB, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVB, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVB, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVB, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVB, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVB, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVB, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVB, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVB, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked_acc( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVH, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVH, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVH, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVH, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVH, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVH, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVH, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVH, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked_acc( VVH, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked_acc( VVH, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVH, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVH, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVH, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVH, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVH, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked_acc( VVH, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVH, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVH, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVH, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVH, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVH, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVH, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVH, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVH, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVH, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVH, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVH, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVH, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVH, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVH, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVH, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVH, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked_acc( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVW, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVW, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVW, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVW, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVW, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVW, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVW, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVW, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked_acc( VVW, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked_acc( VVW, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVW, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVW, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVW, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVW, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVW, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked_acc( VVW, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVW, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVW, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVW, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVW, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVW, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVW, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVW, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVW, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVW, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVW, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVW, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVW, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVW, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVW, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVW, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVW, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_masked_acc( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVB, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked_acc( SVB, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked_acc( SVB, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVB, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVB, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked_acc( SVB, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVB, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVB, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked_acc( SVB, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked_acc( SVB, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked_acc( SVB, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked_acc( SVB, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked_acc( SVB, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked_acc( SVB, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked_acc( SVB, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked_acc( SVB, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVB, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVB, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVB, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVB, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVB, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVB, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVB, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVB, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVB, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVB, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVB, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVB, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVB, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVB, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVB, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVB, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked_acc( SVBU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked_acc( SVBU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked_acc( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVH, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked_acc( SVH, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked_acc( SVH, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVH, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVH, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked_acc( SVH, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVH, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVH, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked_acc( SVH, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked_acc( SVH, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked_acc( SVH, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked_acc( SVH, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked_acc( SVH, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked_acc( SVH, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked_acc( SVH, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked_acc( SVH, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVH, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVH, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVH, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVH, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVH, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVH, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVH, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVH, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVH, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVH, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVH, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVH, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVH, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVH, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVH, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVH, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked_acc( SVHU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked_acc( SVHU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked_acc( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVW, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked_acc( SVW, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked_acc( SVW, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVW, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVW, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked_acc( SVW, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVW, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVW, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked_acc( SVW, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked_acc( SVW, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked_acc( SVW, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked_acc( SVW, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked_acc( SVW, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked_acc( SVW, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked_acc( SVW, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked_acc( SVW, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVW, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVW, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVW, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVW, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVW, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVW, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVW, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVW, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVW, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVW, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVW, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVW, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVW, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVW, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVW, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVW, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_masked_acc( SVWU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_masked_acc( SVWU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_masked_acc( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEB, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEB, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEB, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEB, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEB, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEB, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEB, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEB, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEB, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEB, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEB, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEB, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEB, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEB, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEB, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEB, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEB, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEB, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEB, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEB, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEB, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEB, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEB, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEB, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEB, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEB, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEB, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEB, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEB, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEB, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEB, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEB, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEB, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEB, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEB, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEB, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEB, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEB, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEH, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEH, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEH, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEH, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEH, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEH, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEH, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEH, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEH, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEH, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEH, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEH, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEH, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEH, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEH, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEH, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEH, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEH, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEH, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEH, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEH, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEH, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEH, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEH, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEH, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEH, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEH, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEH, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEH, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEH, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEH, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEH, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEH, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEH, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEH, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEH, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEH, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEH, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEW, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEW, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEW, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEW, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEW, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEW, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEW, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEW, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEW, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEW, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEW, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEW, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEW, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEW, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEW, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEW, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEW, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEW, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEW, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEW, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEW, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEW, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEW, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEW, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEW, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEW, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEW, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEW, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEW, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEW, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEW, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEW, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEW, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEW, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEW, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEW, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEW, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEW, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEB, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEB, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEB, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEB, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEB, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEB, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEB, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEB, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEB, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEB, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEB, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEB, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEB, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEB, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEB, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEB, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEB, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEB, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEB, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEB, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEB, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEB, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEB, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEB, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEB, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEB, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEB, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEB, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEB, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEB, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEB, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEB, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEB, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEB, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEB, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEB, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEB, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEH, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEH, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEH, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEH, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEH, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEH, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEH, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEH, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEH, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEH, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEH, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEH, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEH, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEH, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEH, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEH, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEH, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEH, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEH, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEH, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEH, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEH, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEH, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEH, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEH, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEH, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEH, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEH, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEH, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEH, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEH, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEH, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEH, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEH, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEH, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEH, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEH, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEW, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEW, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEW, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEW, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEW, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEW, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEW, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEW, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEW, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEW, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEW, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEW, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEW, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEW, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEW, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEW, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEW, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEW, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEW, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEW, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEW, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEW, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEW, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEW, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEW, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEW, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEW, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEW, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEW, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEW, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEW, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEW, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEW, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEW, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEW, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEW, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEW, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVB, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_2D( VVB, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_2D( VVB, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_2D( VVB, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVB, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_2D( VVB, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_2D( VVB, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_2D( VVB, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_2D( VVB, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_2D( VVB, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_2D( VVB, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_2D( VVB, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_2D( VVB, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_2D( VVB, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_2D( VVB, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_2D( VVB, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVB, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVB, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVB, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVB, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVB, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVB, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVB, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVB, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVB, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVB, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVB, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVB, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVB, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVB, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVB, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVB, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_2D( VVBU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_2D( VVBU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_2D( VVBU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_2D( VVBU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_2D( VVBU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_2D( VVBU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_2D( VVBU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_2D( VVBU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_2D( VVBU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_2D( VVBU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_2D( VVBU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_2D( VVBU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_2D( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVH, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_2D( VVH, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_2D( VVH, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_2D( VVH, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVH, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_2D( VVH, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_2D( VVH, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_2D( VVH, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_2D( VVH, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_2D( VVH, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_2D( VVH, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_2D( VVH, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_2D( VVH, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_2D( VVH, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_2D( VVH, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_2D( VVH, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVH, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVH, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVH, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVH, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVH, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVH, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVH, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVH, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVH, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVH, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVH, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVH, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVH, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVH, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVH, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVH, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_2D( VVHU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_2D( VVHU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_2D( VVHU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_2D( VVHU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_2D( VVHU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_2D( VVHU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_2D( VVHU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_2D( VVHU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_2D( VVHU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_2D( VVHU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_2D( VVHU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_2D( VVHU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_2D( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVW, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_2D( VVW, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_2D( VVW, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_2D( VVW, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVW, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_2D( VVW, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_2D( VVW, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_2D( VVW, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_2D( VVW, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_2D( VVW, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_2D( VVW, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_2D( VVW, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_2D( VVW, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_2D( VVW, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_2D( VVW, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_2D( VVW, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVW, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVW, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVW, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVW, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVW, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVW, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVW, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVW, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVW, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVW, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVW, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVW, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVW, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVW, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVW, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVW, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_2D( VVWU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_2D( VVWU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_2D( VVWU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_2D( VVWU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_2D( VVWU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_2D( VVWU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_2D( VVWU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_2D( VVWU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_2D( VVWU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_2D( VVWU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_2D( VVWU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_2D( VVWU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_2D( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVB, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_2D( SVB, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_2D( SVB, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_2D( SVB, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_2D( SVB, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_2D( SVB, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_2D( SVB, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_2D( SVB, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_2D( SVB, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_2D( SVB, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_2D( SVB, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_2D( SVB, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_2D( SVB, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_2D( SVB, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_2D( SVB, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_2D( SVB, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVB, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVB, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVB, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVB, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVB, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVB, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVB, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVB, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVB, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVB, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVB, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVB, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVB, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVB, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVB, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVB, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_2D( SVBU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_2D( SVBU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_2D( SVBU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_2D( SVBU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_2D( SVBU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_2D( SVBU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_2D( SVBU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_2D( SVBU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_2D( SVBU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_2D( SVBU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_2D( SVBU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_2D( SVBU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_2D( SVBU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_2D( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVH, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_2D( SVH, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_2D( SVH, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_2D( SVH, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_2D( SVH, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_2D( SVH, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_2D( SVH, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_2D( SVH, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_2D( SVH, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_2D( SVH, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_2D( SVH, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_2D( SVH, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_2D( SVH, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_2D( SVH, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_2D( SVH, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_2D( SVH, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVH, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVH, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVH, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVH, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVH, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVH, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVH, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVH, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVH, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVH, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVH, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVH, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVH, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVH, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVH, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVH, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_2D( SVHU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_2D( SVHU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_2D( SVHU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_2D( SVHU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_2D( SVHU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_2D( SVHU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_2D( SVHU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_2D( SVHU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_2D( SVHU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_2D( SVHU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_2D( SVHU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_2D( SVHU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_2D( SVHU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_2D( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVW, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_2D( SVW, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_2D( SVW, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_2D( SVW, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_2D( SVW, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_2D( SVW, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_2D( SVW, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_2D( SVW, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_2D( SVW, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_2D( SVW, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_2D( SVW, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_2D( SVW, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_2D( SVW, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_2D( SVW, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_2D( SVW, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_2D( SVW, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVW, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVW, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVW, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVW, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVW, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVW, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVW, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVW, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVW, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVW, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVW, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVW, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVW, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVW, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVW, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVW, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_2D( SVWU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_2D( SVWU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_2D( SVWU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_2D( SVWU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_2D( SVWU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_2D( SVWU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_2D( SVWU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_2D( SVWU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_2D( SVWU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_2D( SVWU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_2D( SVWU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_2D( SVWU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_2D( SVWU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_2D( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEB, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEB, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEB, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEB, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEB, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEB, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEB, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEB, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEB, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEB, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEB, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEB, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEB, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEB, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEB, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_2D( VEB, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_2D( VEB, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEB, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEB, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEB, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEB, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEB, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEB, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEB, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEB, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEB, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEB, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEB, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEB, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEB, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEB, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEB, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEB, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEB, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEB, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEB, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEB, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEB, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEH, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEH, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEH, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEH, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEH, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEH, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEH, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEH, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEH, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEH, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEH, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEH, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEH, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEH, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEH, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_2D( VEH, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_2D( VEH, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEH, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEH, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEH, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEH, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEH, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEH, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEH, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEH, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEH, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEH, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEH, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEH, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEH, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEH, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEH, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEH, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEH, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEH, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEH, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEH, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEH, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEW, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEW, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEW, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEW, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEW, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEW, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEW, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEW, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEW, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEW, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEW, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEW, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEW, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEW, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEW, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_2D( VEW, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_2D( VEW, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEW, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEW, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEW, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEW, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEW, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEW, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEW, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEW, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEW, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEW, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEW, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEW, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEW, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEW, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEW, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEW, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEW, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEW, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEW, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEW, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEW, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEB, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEB, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEB, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEB, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEB, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEB, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEB, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEB, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEB, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEB, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEB, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEB, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEB, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEB, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEB, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEB, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEB, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEB, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEB, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEB, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEB, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEB, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEB, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEB, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEB, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEB, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEB, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEB, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEB, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEB, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEB, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEB, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEB, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEB, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEB, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEB, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEB, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEH, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEH, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEH, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEH, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEH, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEH, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEH, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEH, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEH, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEH, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEH, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEH, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEH, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEH, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEH, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEH, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEH, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEH, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEH, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEH, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEH, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEH, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEH, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEH, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEH, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEH, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEH, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEH, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEH, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEH, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEH, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEH, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEH, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEH, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEH, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEH, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEH, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEW, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEW, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEW, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEW, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEW, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEW, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEW, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEW, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEW, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEW, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEW, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEW, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEW, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEW, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEW, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEW, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEW, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEW, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEW, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEW, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEW, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEW, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEW, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEW, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEW, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEW, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEW, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEW, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEW, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEW, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEW, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEW, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEW, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEW, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEW, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEW, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEW, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVB, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVB, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVB, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVB, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVB, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVB, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVB, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVB, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_2D( VVB, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_2D( VVB, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVB, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVB, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVB, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVB, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVB, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_2D( VVB, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVB, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVB, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVB, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVB, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVB, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVB, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVB, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVB, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVB, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVB, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVB, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVB, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVB, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVB, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVB, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVB, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_2D( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVH, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVH, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVH, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVH, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVH, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVH, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVH, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVH, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_2D( VVH, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_2D( VVH, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVH, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVH, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVH, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVH, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVH, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_2D( VVH, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVH, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVH, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVH, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVH, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVH, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVH, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVH, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVH, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVH, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVH, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVH, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVH, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVH, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVH, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVH, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVH, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_2D( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVW, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVW, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVW, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVW, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVW, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVW, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVW, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVW, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_2D( VVW, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_2D( VVW, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVW, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVW, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVW, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVW, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVW, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_2D( VVW, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVW, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVW, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVW, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVW, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVW, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVW, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVW, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVW, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVW, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVW, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVW, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVW, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVW, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVW, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVW, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVW, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_2D( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVB, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_2D( SVB, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_2D( SVB, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVB, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVB, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_2D( SVB, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVB, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVB, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_2D( SVB, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_2D( SVB, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_2D( SVB, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_2D( SVB, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_2D( SVB, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_2D( SVB, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_2D( SVB, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_2D( SVB, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVB, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVB, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVB, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVB, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVB, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVB, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVB, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVB, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVB, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVB, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVB, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVB, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVB, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVB, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVB, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVB, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_2D( SVBU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_2D( SVBU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_2D( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVH, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_2D( SVH, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_2D( SVH, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVH, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVH, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_2D( SVH, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVH, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVH, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_2D( SVH, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_2D( SVH, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_2D( SVH, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_2D( SVH, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_2D( SVH, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_2D( SVH, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_2D( SVH, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_2D( SVH, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVH, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVH, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVH, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVH, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVH, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVH, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVH, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVH, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVH, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVH, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVH, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVH, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVH, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVH, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVH, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVH, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_2D( SVHU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_2D( SVHU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_2D( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVW, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_2D( SVW, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_2D( SVW, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVW, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVW, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_2D( SVW, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVW, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVW, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_2D( SVW, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_2D( SVW, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_2D( SVW, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_2D( SVW, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_2D( SVW, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_2D( SVW, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_2D( SVW, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_2D( SVW, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVW, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVW, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVW, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVW, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVW, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVW, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVW, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVW, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVW, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVW, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVW, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVW, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVW, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVW, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVW, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVW, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_2D( SVWU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_2D( SVWU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_2D( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEB, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEB, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEB, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEB, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEB, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEB, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEB, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEB, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEB, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEB, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEB, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEB, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEB, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEB, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEB, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEB, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEB, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEB, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEB, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEB, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEB, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEB, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEB, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEB, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEB, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEB, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEB, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEB, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEB, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEB, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEB, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEB, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEB, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEB, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEB, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEB, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEB, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEB, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEH, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEH, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEH, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEH, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEH, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEH, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEH, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEH, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEH, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEH, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEH, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEH, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEH, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEH, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEH, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEH, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEH, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEH, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEH, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEH, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEH, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEH, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEH, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEH, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEH, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEH, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEH, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEH, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEH, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEH, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEH, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEH, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEH, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEH, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEH, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEH, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEH, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEH, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEW, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEW, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEW, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEW, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEW, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEW, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEW, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEW, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEW, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEW, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEW, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEW, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEW, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEW, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEW, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEW, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEW, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEW, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEW, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEW, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEW, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEW, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEW, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEW, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEW, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEW, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEW, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEW, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEW, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEW, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEW, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEW, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEW, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEW, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEW, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEW, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEW, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEW, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEB, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEB, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEB, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEB, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEB, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEB, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEB, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEB, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEB, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEB, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEB, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEB, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEB, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEB, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEB, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEB, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEB, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEB, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEB, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEB, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEB, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEB, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEB, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEB, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEB, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEB, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEB, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEB, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEB, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEB, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEB, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEB, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEB, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEB, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEB, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEB, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEB, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEH, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEH, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEH, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEH, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEH, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEH, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEH, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEH, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEH, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEH, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEH, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEH, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEH, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEH, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEH, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEH, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEH, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEH, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEH, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEH, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEH, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEH, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEH, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEH, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEH, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEH, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEH, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEH, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEH, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEH, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEH, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEH, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEH, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEH, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEH, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEH, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEH, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEW, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEW, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEW, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEW, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEW, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEW, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEW, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEW, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEW, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEW, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEW, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEW, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEW, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEW, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEW, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEW, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEW, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEW, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEW, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEW, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEW, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEW, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEW, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEW, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEW, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEW, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEW, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEW, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEW, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEW, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEW, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEW, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEW, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEW, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEW, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEW, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEW, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVB, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_3D( VVB, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_3D( VVB, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_3D( VVB, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVB, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_3D( VVB, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_3D( VVB, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_3D( VVB, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_3D( VVB, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_3D( VVB, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_3D( VVB, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_3D( VVB, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_3D( VVB, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_3D( VVB, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_3D( VVB, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_3D( VVB, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVB, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVB, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVB, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVB, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVB, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVB, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVB, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVB, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVB, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVB, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVB, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVB, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVB, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVB, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVB, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVB, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_3D( VVBU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_3D( VVBU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_3D( VVBU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_3D( VVBU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_3D( VVBU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_3D( VVBU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_3D( VVBU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_3D( VVBU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_3D( VVBU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_3D( VVBU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_3D( VVBU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_3D( VVBU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_3D( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVH, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_3D( VVH, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_3D( VVH, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_3D( VVH, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVH, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_3D( VVH, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_3D( VVH, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_3D( VVH, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_3D( VVH, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_3D( VVH, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_3D( VVH, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_3D( VVH, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_3D( VVH, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_3D( VVH, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_3D( VVH, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_3D( VVH, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVH, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVH, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVH, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVH, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVH, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVH, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVH, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVH, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVH, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVH, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVH, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVH, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVH, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVH, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVH, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVH, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_3D( VVHU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_3D( VVHU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_3D( VVHU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_3D( VVHU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_3D( VVHU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_3D( VVHU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_3D( VVHU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_3D( VVHU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_3D( VVHU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_3D( VVHU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_3D( VVHU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_3D( VVHU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_3D( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVW, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_3D( VVW, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_3D( VVW, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_3D( VVW, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVW, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_3D( VVW, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_3D( VVW, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_3D( VVW, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_3D( VVW, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_3D( VVW, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_3D( VVW, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_3D( VVW, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_3D( VVW, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_3D( VVW, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_3D( VVW, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_3D( VVW, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVW, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVW, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVW, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVW, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVW, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVW, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVW, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVW, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVW, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVW, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVW, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVW, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVW, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVW, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVW, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVW, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_3D( VVWU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_3D( VVWU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_3D( VVWU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_3D( VVWU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_3D( VVWU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_3D( VVWU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_3D( VVWU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_3D( VVWU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_3D( VVWU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_3D( VVWU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_3D( VVWU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_3D( VVWU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_3D( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVB, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_3D( SVB, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_3D( SVB, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_3D( SVB, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_3D( SVB, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_3D( SVB, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_3D( SVB, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_3D( SVB, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_3D( SVB, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_3D( SVB, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_3D( SVB, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_3D( SVB, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_3D( SVB, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_3D( SVB, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_3D( SVB, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_3D( SVB, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVB, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVB, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVB, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVB, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVB, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVB, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVB, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVB, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVB, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVB, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVB, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVB, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVB, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVB, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVB, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVB, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_3D( SVBU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_3D( SVBU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_3D( SVBU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_3D( SVBU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_3D( SVBU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_3D( SVBU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_3D( SVBU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_3D( SVBU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_3D( SVBU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_3D( SVBU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_3D( SVBU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_3D( SVBU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_3D( SVBU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_3D( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVH, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_3D( SVH, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_3D( SVH, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_3D( SVH, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_3D( SVH, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_3D( SVH, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_3D( SVH, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_3D( SVH, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_3D( SVH, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_3D( SVH, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_3D( SVH, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_3D( SVH, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_3D( SVH, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_3D( SVH, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_3D( SVH, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_3D( SVH, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVH, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVH, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVH, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVH, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVH, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVH, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVH, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVH, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVH, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVH, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVH, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVH, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVH, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVH, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVH, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVH, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_3D( SVHU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_3D( SVHU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_3D( SVHU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_3D( SVHU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_3D( SVHU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_3D( SVHU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_3D( SVHU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_3D( SVHU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_3D( SVHU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_3D( SVHU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_3D( SVHU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_3D( SVHU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_3D( SVHU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_3D( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVW, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_3D( SVW, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_3D( SVW, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_3D( SVW, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_3D( SVW, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_3D( SVW, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_3D( SVW, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_3D( SVW, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_3D( SVW, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_3D( SVW, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_3D( SVW, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_3D( SVW, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_3D( SVW, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_3D( SVW, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_3D( SVW, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_3D( SVW, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVW, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVW, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVW, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVW, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVW, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVW, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVW, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVW, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVW, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVW, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVW, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVW, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVW, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVW, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVW, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVW, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_3D( SVWU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_3D( SVWU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_3D( SVWU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_3D( SVWU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_3D( SVWU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_3D( SVWU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_3D( SVWU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_3D( SVWU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_3D( SVWU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_3D( SVWU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_3D( SVWU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_3D( SVWU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_3D( SVWU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_3D( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEB, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEB, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEB, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEB, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEB, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEB, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEB, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEB, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEB, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEB, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEB, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEB, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEB, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEB, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEB, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_3D( VEB, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_3D( VEB, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEB, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEB, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEB, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEB, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEB, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEB, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEB, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEB, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEB, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEB, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEB, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEB, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEB, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEB, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEB, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEB, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEB, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEB, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEB, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEB, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEB, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEH, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEH, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEH, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEH, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEH, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEH, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEH, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEH, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEH, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEH, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEH, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEH, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEH, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEH, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEH, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_3D( VEH, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_3D( VEH, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEH, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEH, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEH, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEH, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEH, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEH, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEH, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEH, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEH, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEH, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEH, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEH, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEH, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEH, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEH, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEH, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEH, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEH, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEH, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEH, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEH, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEW, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEW, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEW, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEW, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEW, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEW, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEW, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEW, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEW, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEW, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEW, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEW, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEW, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEW, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEW, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_3D( VEW, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_3D( VEW, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEW, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEW, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEW, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEW, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEW, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEW, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEW, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEW, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEW, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEW, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEW, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEW, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEW, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEW, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEW, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEW, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEW, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEW, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEW, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEW, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEW, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEB, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEB, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEB, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEB, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEB, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEB, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEB, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEB, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEB, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEB, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEB, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEB, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEB, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEB, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEB, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEB, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEB, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEB, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEB, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEB, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEB, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEB, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEB, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEB, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEB, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEB, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEB, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEB, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEB, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEB, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEB, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEB, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEB, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEB, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEB, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEB, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEB, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEH, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEH, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEH, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEH, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEH, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEH, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEH, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEH, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEH, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEH, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEH, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEH, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEH, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEH, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEH, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEH, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEH, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEH, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEH, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEH, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEH, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEH, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEH, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEH, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEH, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEH, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEH, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEH, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEH, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEH, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEH, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEH, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEH, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEH, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEH, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEH, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEH, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEW, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEW, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEW, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEW, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEW, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEW, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEW, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEW, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEW, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEW, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEW, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEW, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEW, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEW, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEW, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEW, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEW, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEW, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEW, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEW, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEW, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEW, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEW, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEW, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEW, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEW, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEW, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEW, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEW, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEW, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEW, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEW, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEW, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEW, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEW, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEW, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEW, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVB, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVB, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVB, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVB, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVB, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVB, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVB, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVB, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_3D( VVB, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_3D( VVB, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVB, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVB, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVB, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVB, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVB, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_3D( VVB, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVB, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVB, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVB, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVB, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVB, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVB, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVB, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVB, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVB, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVB, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVB, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVB, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVB, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVB, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVB, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVB, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_3D( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVH, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVH, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVH, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVH, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVH, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVH, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVH, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVH, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_3D( VVH, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_3D( VVH, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVH, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVH, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVH, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVH, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVH, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_3D( VVH, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVH, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVH, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVH, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVH, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVH, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVH, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVH, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVH, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVH, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVH, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVH, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVH, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVH, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVH, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVH, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVH, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_3D( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHW, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWB, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWB, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWB, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWB, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWB, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWB, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWB, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWB, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWB, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWB, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWB, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWB, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWB, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWB, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWB, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWB, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWB, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWB, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWB, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWB, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWB, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWB, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWB, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWB, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWB, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWB, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWB, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWB, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWB, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWB, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWB, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWB, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWB, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWB, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWB, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWB, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWB, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWB, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWH, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWH, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWH, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWH, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWH, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWH, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWH, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWH, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWH, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWH, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWH, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWH, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWH, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWH, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWH, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWH, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWH, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWH, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWH, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWH, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWH, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWH, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWH, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWH, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWH, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWH, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWH, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWH, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWH, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWH, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWH, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWH, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWH, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWH, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWH, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWH, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWH, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWH, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVW, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVW, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVW, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVW, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVW, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVW, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVW, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVW, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVW, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVW, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVW, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVW, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVW, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVW, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVW, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVW, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVW, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVW, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVW, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVW, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVW, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVW, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVW, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVW, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVW, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVW, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVW, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVW, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVW, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVW, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVW, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVW, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVW, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVW, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVW, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVW, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVW, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVW, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVW, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVW, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVW, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVW, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVW, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVW, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVW, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_3D( VVW, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_3D( VVW, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVW, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVW, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVW, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVW, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVW, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_3D( VVW, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVW, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVW, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVW, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVW, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVW, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVW, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVW, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVW, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVW, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVW, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVW, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVW, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVW, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVW, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVW, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVW, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWU, VADD, v_out, v_out, v_in1 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWU, VSUB, v_out, v_out, v_in1 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWU, VADDC, v_out, v_out, v_in1 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWU, VSUBB, v_out, v_out, v_in1 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWU, VABSDIFF, v_out, v_out, v_in1 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWU, VMUL, v_out, v_out, v_in1 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWU, VMULHI, v_out, v_out, v_in1 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWU, VMULFXP, v_out, v_out, v_in1 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWU, VAND, v_out, v_out, v_in1 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWU, VOR, v_out, v_out, v_in1 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWU, VXOR, v_out, v_out, v_in1 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWU, VSHL, v_out, v_out, v_in1 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWU, VSHR, v_out, v_out, v_in1 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWU, VROTL, v_out, v_out, v_in1 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWU, VROTR, v_out, v_out, v_in1 );
		break;
	case VMOV:
		vbxasm_acc_3D( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWU, VCUSTOM0, v_out, v_out, v_in1 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWU, VCUSTOM1, v_out, v_out, v_in1 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWU, VCUSTOM2, v_out, v_out, v_in1 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWU, VCUSTOM3, v_out, v_out, v_in1 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWU, VCUSTOM4, v_out, v_out, v_in1 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWU, VCUSTOM5, v_out, v_out, v_in1 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWU, VCUSTOM6, v_out, v_out, v_in1 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWU, VCUSTOM7, v_out, v_out, v_in1 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWU, VCUSTOM8, v_out, v_out, v_in1 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWU, VCUSTOM9, v_out, v_out, v_in1 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWU, VCUSTOM10, v_out, v_out, v_in1 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWU, VCUSTOM11, v_out, v_out, v_in1 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWU, VCUSTOM12, v_out, v_out, v_in1 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWU, VCUSTOM13, v_out, v_out, v_in1 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWU, VCUSTOM14, v_out, v_out, v_in1 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWU, VCUSTOM15, v_out, v_out, v_in1 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVB, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_3D( SVB, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_3D( SVB, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVB, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVB, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_3D( SVB, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVB, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVB, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_3D( SVB, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_3D( SVB, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_3D( SVB, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_3D( SVB, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_3D( SVB, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_3D( SVB, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_3D( SVB, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_3D( SVB, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVB, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVB, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVB, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVB, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVB, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVB, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVB, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVB, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVB, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVB, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVB, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVB, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVB, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVB, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVB, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVB, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_3D( SVBU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_3D( SVBU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_3D( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVH, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_3D( SVH, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_3D( SVH, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVH, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVH, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_3D( SVH, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVH, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVH, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_3D( SVH, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_3D( SVH, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_3D( SVH, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_3D( SVH, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_3D( SVH, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_3D( SVH, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_3D( SVH, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_3D( SVH, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVH, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVH, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVH, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVH, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVH, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVH, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVH, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVH, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVH, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVH, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVH, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVH, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVH, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVH, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVH, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVH, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_3D( SVHU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_3D( SVHU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_3D( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWB, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWB, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWB, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWB, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWB, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWB, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWB, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWB, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWB, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWB, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWB, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWB, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWB, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWB, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWB, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWB, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWB, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWB, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWB, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWB, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWB, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWB, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWB, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWB, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWB, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWB, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWB, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWB, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWB, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWB, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWB, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWB, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWB, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWB, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWB, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWB, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWB, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWH, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWH, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWH, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWH, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWH, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWH, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWH, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWH, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWH, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWH, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWH, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWH, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWH, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWH, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWH, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWH, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWH, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWH, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWH, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWH, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWH, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWH, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWH, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWH, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWH, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWH, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWH, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWH, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWH, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWH, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWH, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWH, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWH, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWH, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWH, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWH, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWH, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVW, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVW, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVW, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVW, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVW, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVW, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVW, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVW, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVW, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVW, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVW, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVW, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVW, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVW, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVW, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVW, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVW, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVW, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVW, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVW, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVW, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVW, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVW, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVW, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVW, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVW, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVW, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVW, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVW, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVW, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVW, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVW, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVW, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVW, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVW, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVW, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVW, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVW, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_3D( SVW, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_3D( SVW, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVW, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVW, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_3D( SVW, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVW, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVW, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_3D( SVW, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_3D( SVW, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_3D( SVW, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_3D( SVW, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_3D( SVW, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_3D( SVW, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_3D( SVW, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_3D( SVW, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVW, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVW, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVW, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVW, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVW, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVW, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVW, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVW, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVW, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVW, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVW, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVW, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVW, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVW, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVW, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVW, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWU, VADD, v_out, s_in1, v_out );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWU, VSUB, v_out, s_in1, v_out );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWU, VADDC, v_out, s_in1, v_out );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWU, VSUBB, v_out, s_in1, v_out );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWU, VABSDIFF, v_out, s_in1, v_out );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWU, VMUL, v_out, s_in1, v_out );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWU, VMULHI, v_out, s_in1, v_out );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWU, VMULFXP, v_out, s_in1, v_out );
		break;
	case VAND:
		vbxasm_acc_3D( SVWU, VAND, v_out, s_in1, v_out );
		break;
	case VOR:
		vbxasm_acc_3D( SVWU, VOR, v_out, s_in1, v_out );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWU, VXOR, v_out, s_in1, v_out );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWU, VSHL, v_out, s_in1, v_out );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWU, VSHR, v_out, s_in1, v_out );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWU, VROTL, v_out, s_in1, v_out );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWU, VROTR, v_out, s_in1, v_out );
		break;
	case VMOV:
		vbxasm_acc_3D( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWU, VCUSTOM0, v_out, s_in1, v_out );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWU, VCUSTOM1, v_out, s_in1, v_out );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWU, VCUSTOM2, v_out, s_in1, v_out );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWU, VCUSTOM3, v_out, s_in1, v_out );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWU, VCUSTOM4, v_out, s_in1, v_out );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWU, VCUSTOM5, v_out, s_in1, v_out );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWU, VCUSTOM6, v_out, s_in1, v_out );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWU, VCUSTOM7, v_out, s_in1, v_out );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWU, VCUSTOM8, v_out, s_in1, v_out );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWU, VCUSTOM9, v_out, s_in1, v_out );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWU, VCUSTOM10, v_out, s_in1, v_out );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWU, VCUSTOM11, v_out, s_in1, v_out );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWU, VCUSTOM12, v_out, s_in1, v_out );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWU, VCUSTOM13, v_out, s_in1, v_out );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWU, VCUSTOM14, v_out, s_in1, v_out );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWU, VCUSTOM15, v_out, s_in1, v_out );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEB, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEB, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEB, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEB, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEB, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEB, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEB, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEB, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEB, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEB, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEB, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEB, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEB, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEB, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEB, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEB, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEB, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEB, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEB, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEB, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEB, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEB, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEB, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEB, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEB, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEB, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEB, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEB, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEB, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEB, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEB, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEB, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEB, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEB, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEB, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEB, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEB, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEB, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEH, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEH, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEH, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEH, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEH, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEH, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEH, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEH, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEH, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEH, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEH, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEH, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEH, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEH, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEH, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEH, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEH, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEH, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEH, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEH, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEH, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEH, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEH, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEH, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEH, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEH, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEH, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEH, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEH, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEH, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEH, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEH, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEH, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEH, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEH, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEH, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEH, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEH, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHW, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWB, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWB, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWB, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWB, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWB, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWB, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWB, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWB, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWB, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWB, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWB, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWB, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWB, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWB, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWB, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWB, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWB, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWB, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWB, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWB, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWB, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWB, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWB, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWB, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWB, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWB, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWB, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWB, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWB, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWB, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWB, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWH, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWH, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWH, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWH, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWH, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWH, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWH, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWH, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWH, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWH, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWH, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWH, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWH, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWH, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWH, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWH, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWH, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWH, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWH, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWH, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWH, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWH, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWH, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWH, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWH, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWH, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWH, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWH, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWH, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWH, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWH, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEW, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEW, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEW, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEW, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEW, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEW, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEW, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEW, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEW, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEW, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEW, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEW, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEW, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEW, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEW, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEW, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEW, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEW, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEW, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEW, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEW, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEW, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEW, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEW, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEW, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEW, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEW, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEW, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEW, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEW, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEW, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEW, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEW, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEW, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEW, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEW, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEW, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEW, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEW, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEW, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEW, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEW, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEW, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEW, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEW, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEW, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEW, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEW, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEW, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEW, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEW, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEW, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEW, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEW, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEW, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEW, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEW, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEW, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEW, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEW, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEW, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEW, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEW, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEW, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEW, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEW, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEW, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEW, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWU, VADD, v_out, v_out, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWU, VSUB, v_out, v_out, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWU, VADDC, v_out, v_out, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWU, VSUBB, v_out, v_out, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWU, VABSDIFF, v_out, v_out, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWU, VMUL, v_out, v_out, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWU, VMULHI, v_out, v_out, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWU, VMULFXP, v_out, v_out, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWU, VAND, v_out, v_out, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWU, VOR, v_out, v_out, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWU, VXOR, v_out, v_out, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWU, VSHL, v_out, v_out, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWU, VSHR, v_out, v_out, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWU, VROTL, v_out, v_out, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWU, VROTR, v_out, v_out, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWU, VCUSTOM0, v_out, v_out, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWU, VCUSTOM1, v_out, v_out, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWU, VCUSTOM2, v_out, v_out, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWU, VCUSTOM3, v_out, v_out, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWU, VCUSTOM4, v_out, v_out, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWU, VCUSTOM5, v_out, v_out, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWU, VCUSTOM6, v_out, v_out, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWU, VCUSTOM7, v_out, v_out, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWU, VCUSTOM8, v_out, v_out, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWU, VCUSTOM9, v_out, v_out, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWU, VCUSTOM10, v_out, v_out, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWU, VCUSTOM11, v_out, v_out, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWU, VCUSTOM12, v_out, v_out, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWU, VCUSTOM13, v_out, v_out, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWU, VCUSTOM14, v_out, v_out, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWU, VCUSTOM15, v_out, v_out, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEB, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEB, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEB, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEB, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEB, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEB, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEB, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEB, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEB, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEB, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEB, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEB, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEB, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEB, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEB, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEB, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEB, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEB, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEB, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEB, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEB, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEB, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEB, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEB, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEB, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEB, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEB, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEB, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEB, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEB, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEB, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEB, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEB, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEB, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEB, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEB, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEB, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEH, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEH, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEH, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEH, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEH, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEH, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEH, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEH, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEH, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEH, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEH, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEH, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEH, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEH, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEH, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEH, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEH, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEH, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEH, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEH, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEH, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEH, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEH, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEH, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEH, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEH, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEH, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEH, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEH, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEH, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEH, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEH, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEH, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEH, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEH, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEH, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEH, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEW, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEW, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEW, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEW, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEW, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEW, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEW, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEW, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEW, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEW, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEW, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEW, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEW, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEW, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEW, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEW, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEW, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEW, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEW, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEW, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEW, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEW, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEW, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEW, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEW, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEW, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEW, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEW, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEW, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEW, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEW, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEW, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEW, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEW, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEW, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEW, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEW, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}


#endif // __VBXX_HPP
